# AI Service Prompt Caching Implementation Plan

## Overview

This document outlines the plan to implement prompt caching in the AI Service (`src/main/ai-service.ts`) to reduce token costs and improve performance when using Anthropic models through the AI SDK.

## Current Implementation Analysis

The AI Service currently constructs messages for each request without caching:

1. **System Message Generation** (`getSystemMessage()`)
   - Static system prompt loaded from file
   - Dynamic contextual information (date, timezone)
   - Note type information loaded from NoteService
   - Reconstructed on every request

2. **Conversation History Management**
   - Messages stored in `conversationHistories` Map
   - History appended to each request
   - No caching of stable message segments

3. **Message Flow**
   - `sendMessage()` and `sendMessageStream()` both reconstruct full message array
   - System message + conversation history sent every time
   - High token usage for repeated stable content

## Caching Opportunities

### 1. System Message Caching (High Priority)
**Target Content:**
- System prompt (static, loaded from file)
- Contextual information (date/timezone - stable within day)
- Note type information (changes infrequently)

**Benefits:**
- Reduces ~500-2000 tokens per request
- Cache valid for entire day or until note types change
- High cache hit rate for active sessions

### 2. Conversation History Caching (Medium Priority)
**Target Content:**
- Earlier messages in long conversations (5+ message pairs)
- Cache stable portions, keep recent 2-3 messages uncached
- Particularly beneficial for troubleshooting sessions

**Benefits:**
- Significant savings for long conversations
- Reduces cumulative token costs over conversation lifetime

### 3. Note Type Schema Caching (Low Priority)
**Target Content:**
- Tool schemas generated by `getFlintToolSchemas()`
- Static unless note types are modified

**Benefits:**
- Reduces tool definition overhead
- Stable across multiple conversations

## Implementation Strategy

### Phase 1: System Message Caching

#### 1.1 Modify Message Structure
```typescript
interface CacheableMessage extends ModelMessage {
  cacheControl?: {
    type: 'ephemeral';
  };
}
```

#### 1.2 Update getSystemMessage()
- Add optional `enableCaching` parameter
- Return message with cache control metadata when enabled
- Ensure content meets minimum token threshold (1024+ tokens)

#### 1.3 Cache Control Logic
```typescript
private async getSystemMessageWithCaching(): Promise<ModelMessage> {
  const content = await this.getSystemMessage();
  
  // Only cache if content is substantial enough
  if (content.length > 3000) { // Rough token estimate
    return {
      role: 'system',
      content: [
        {
          type: 'text',
          text: content,
          providerOptions: {
            anthropic: { 
              cacheControl: { type: 'ephemeral' } 
            }
          }
        }
      ]
    };
  }
  
  return { role: 'system', content };
}
```

### Phase 2: Conversation History Caching

#### 2.1 Implement History Segmentation
- Identify stable message segments (older than last 3 messages)
- Cache segments that meet minimum token requirements
- Keep recent messages uncached for flexibility

#### 2.2 Cache Strategy
```typescript
private prepareCachedMessages(history: ModelMessage[]): ModelMessage[] {
  if (history.length <= 6) return history; // Too short to benefit
  
  const stableMessages = history.slice(0, -4); // All but last 4 messages
  const recentMessages = history.slice(-4);   // Last 4 messages
  
  // Cache stable portion if substantial
  if (this.estimateTokens(stableMessages) > 1024) {
    return [
      this.createCachedMessageSegment(stableMessages),
      ...recentMessages
    ];
  }
  
  return history;
}
```

### Phase 3: Monitoring and Optimization

#### 3.1 Cache Metrics
- Track cache hit rates via `result.providerMetadata.anthropic`
- Monitor token savings
- Log cache performance for optimization

#### 3.2 Configuration Options
```typescript
interface CacheConfig {
  enableSystemMessageCaching: boolean;
  enableHistoryCaching: boolean;
  minimumCacheTokens: number;
  historySegmentSize: number;
}
```

## Technical Requirements

### AI SDK Integration
- Use `providerOptions.anthropic.cacheControl` for cache control
- Set `{ type: 'ephemeral' }` for session-based caching
- Ensure content meets minimum token thresholds:
  - Claude 3.5 Sonnet: 1024 tokens minimum
  - Claude 3 Haiku: 2048 tokens minimum

### Model Compatibility
- Primary target: Anthropic models (`anthropic/*`)
- Graceful degradation for non-Anthropic models
- Cache control only applied when using compatible providers

### Error Handling
- Handle cache failures gracefully
- Fallback to non-cached requests if cache control fails
- Log cache-related errors for debugging

## Implementation Plan

### Step 1: Basic System Message Caching
1. Add cache control types and interfaces
2. Modify `getSystemMessage()` to support caching
3. Update `sendMessage()` and `sendMessageStream()` to use cached system messages
4. Test with Anthropic models

### Step 2: Conversation History Caching
1. Implement message segmentation logic
2. Add history caching to message preparation
3. Test with long conversations
4. Monitor token savings

### Step 3: Monitoring and Configuration
1. Add cache metrics collection
2. Implement configuration options
3. Add logging for cache performance
4. Create admin interface for cache settings

### Step 4: Optimization
1. Fine-tune cache thresholds based on real usage
2. Implement cache invalidation strategies
3. Add cache warming for frequently used content
4. Optimize cache key generation

## Expected Benefits

### Token Cost Reduction
- **System Messages**: 30-50% reduction in system prompt tokens
- **Long Conversations**: 20-40% reduction in total conversation tokens
- **Active Sessions**: Cumulative savings increase over session duration

### Performance Improvements
- Reduced latency for cached content
- Lower API costs for high-volume usage
- Better resource utilization

### User Experience
- Faster response times for cached requests
- More cost-effective long conversations
- Improved scalability for multiple concurrent users

## Risks and Mitigation

### Cache Invalidation
- **Risk**: Stale cached content
- **Mitigation**: Implement TTL and content-based invalidation

### Token Estimation
- **Risk**: Inaccurate token counting for cache thresholds
- **Mitigation**: Conservative estimates, real-world testing

### Model Compatibility
- **Risk**: Cache control breaking with non-Anthropic models
- **Mitigation**: Provider detection and graceful degradation

### Debugging Complexity
- **Risk**: Harder to debug cached vs. non-cached requests
- **Mitigation**: Comprehensive logging and cache metadata tracking

## Success Metrics

1. **Token Reduction**: 25%+ reduction in total tokens for typical usage patterns
2. **Cache Hit Rate**: 60%+ hit rate for system messages
3. **Performance**: 10-20% improvement in response latency for cached content
4. **Cost Savings**: Measurable reduction in API costs for active users
5. **Reliability**: No increase in error rates or degraded functionality